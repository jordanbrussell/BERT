{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: BERT Twitter Market Prediction\n",
    "\n",
    "### Jordan Bickelhaupt, Evelina Ramoskaite, Dave Sawh, Andrew Schiek\n",
    "###### jordanrbickelhaupt@gmail.com, evelina.ramoskaite@gmail.com, devindrasawh@gmail.com, schiekandrew@gmail.com\n",
    "\n",
    "\n",
    "Data: https://www.kaggle.com/datasets/kazanova/sentiment140\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2B9-HFJBOyq"
   },
   "source": [
    "### Imports & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13986,
     "status": "ok",
     "timestamp": 1603110914115,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "oXO5FPswBLSH",
    "outputId": "f66742a4-8523-4c8d-fb39-ec85782057bd"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install tensorflow_hub==0.9.0\n",
    "# !pip install keras tf-models-official pydot graphviz\n",
    "# !pip install yfinance\n",
    "# !pip install bert-tensorflow==1.0.1\n",
    "# !pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.optimization as op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jbickelhaupt\\\\Downloads\\\\BERT-main (4)\\\\BERT-main'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.8.8 (default, Apr 13 2021, 19:58:26) \n",
    "[GCC 7.3.0]\n",
    "\n",
    "'/home/jordan/rot/ent/tfBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 18070,
     "status": "ok",
     "timestamp": 1603110921824,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "kHzoXf8sCxaW",
    "outputId": "bc00c477-cc18-4c98-cb56-2fa4be35218a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.8.3\n",
      "Eager mode:  True\n",
      "Hub version:  0.9.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# import official.nlp.bert.bert_models\n",
    "# import official.nlp.bert.configs\n",
    "# import official.nlp.bert.run_classifier\n",
    "# import official.nlp.bert.tokenization as tokenization\n",
    "import bert.tokenization as tokenization\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "# from official.nlp import bert\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWfuCNEnDJSX"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If__U1DMDSoF"
   },
   "source": [
    "##### Sentiment 140 dataset\n",
    "https://www.tensorflow.org/datasets/catalog/sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk5dDjqpDT7A"
   },
   "source": [
    "Context \\\\\n",
    "\\\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment. \\\\\n",
    "\\\n",
    "Content \\\\\n",
    "\\\n",
    "It contains the following 6 fields:\n",
    "* target: the polarity of the tweet (0 = negative, 4 = positive)\n",
    "\n",
    "* ids: The id of the tweet ( 2087)\n",
    "\n",
    "* date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "* flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "* user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "* text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "Acknowledgements \\\\\n",
    "The official link regarding the dataset with resources about how it was generated is [here](http://%20http//help.sentiment140.com/for-students/) \\\\\n",
    "The official paper detailing the approach is [here](http://bhttp//cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8Iwpl2fpCwFq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "df = pd.read_csv('sent140.csv',                                           \n",
    "                 encoding='ISO-8859-1', \n",
    "                 names=[\n",
    "                        'target',\n",
    "                        'id',\n",
    "                        'date',\n",
    "                        'flag',\n",
    "                        'user',\n",
    "                        'text'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCwKlyvtD2wB"
   },
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e51xO2BD8wZ"
   },
   "source": [
    "We don't really need all 1.6 Million tweets for training so we can take a sample of 5% (to save up time on training) and then split that into 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "I2bcU8X_Dz9M"
   },
   "outputs": [],
   "source": [
    "df['wkd'] = [i[0:3] for i in df['date']]\n",
    "\n",
    "\n",
    "sample_size = int(len(df)*0.0005)\n",
    "sampleDf = df.sample(sample_size, random_state=23)\n",
    "wkd = sampleDf.wkd.values\n",
    "x = sampleDf.text.values\n",
    "y = sampleDf.target.values\n",
    "wkd_train, wkd_test, x_train, x_test, y_train, y_test = train_test_split(wkd, x, y, test_size=0.20, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "# If np_utils fails to load, rerun cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6CgL-y4EI3o"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNDz2KRGEMRS"
   },
   "source": [
    "We could get away with other approach but you might want to use this for more than binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0mmW4jGVDmCH"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y_test = encoder.transform(y_test)\n",
    "encoded_Y_train = encoder.transform(y_train)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "dummy_y_train = np_utils.to_categorical(encoded_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doKxHdH6EXNZ"
   },
   "source": [
    "We might want to use encoding for later. For that we can save enconding map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cwQNtv17EdUz"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "my_wd = 'C:\\\\Users\\\\jbickelhaupt\\\\Downloads\\\\BERT-main (4)\\\\BERT-main'\n",
    "np.save(os.path.join(my_wd, encoder_fname) , encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct1-P5r1FGQO"
   },
   "source": [
    "To load it when you'll use this in production just use the below cell (uncommented ofcourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HEopM33sE_Et"
   },
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq5GSy35Gibn"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5f_ey3VGknK"
   },
   "source": [
    "To tokenize our text we will use some functions from official.nlp.bert package and the pretrained BERT model itself. \\\\\n",
    "First we get the BERT model. I'll use the multilingual one as it will be best for most of your cases. To check all NLP models provided by TF HUB go [here](https://tfhub.dev/s?module-type=text-embedding,text-classification,text-generation,text-language-model,text-question-answering,text-retrieval-question-answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "atMa7VWVFQwV"
   },
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VoD6yod_FRyN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh9FX32qHeKI"
   },
   "source": [
    "You can see that in above cell we have loaded some variables using the bert_layer we have downloaded. \n",
    "\n",
    "1.   ```vocab_file``` reads the vocab file associated to the downloaded model.\n",
    "2.   ```do_lower_case``` reads binary variable which if ```True``` means tokenizer will reformat all text to lower case rendering model to be **NOT** case sensitive. Should be ```False``` by default. You can check in a cell below.\n",
    "3.   ```tokenizer``` builds tokenizer using variables 1 and 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3lCeedHKkkB"
   },
   "source": [
    "Lastly we need to add two additional tokens: Classification and Seperation. We will add these through functions we will use to tokenize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 10119,
     "status": "ok",
     "timestamp": 1602562167723,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "q1Y7q4QTKuTD",
    "outputId": "7fd94ebd-8cab-4abc-b326-ac5b8b18de08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xazos3SQKa8w"
   },
   "source": [
    "Kewl, by this point we have everything for tokenization. This will take a bit as we have quite a bit of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkRhOGvoSrTi"
   },
   "source": [
    "## Prep inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iJXVNBRSwWz"
   },
   "source": [
    "So since we went extensively through input prep in last video where we finetune non modified BERT model, this time we'll jump straight to input prep through functions. Everything that will be different from the old notebook will have **[NEW]** next to it. Our inputs are:\n",
    "\n",
    "1.   Tokens\n",
    "2.   Input mask\n",
    "3.   Input type\n",
    "4.   **[NEW]** Weekday\n",
    "\n",
    "Remember when I said it will make our work easier both now and in the future if we remake everything into funcitons? Now is that future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0eyZii6TAwo"
   },
   "source": [
    "First let's set the max sequence lenght as we did in simple NN example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e6gshuXmRwJ8"
   },
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "tweets = tf.ragged.constant([encode_names(n, tokenizer) for n in x_train])\n",
    "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tweets.shape[0]\n",
    "input_word_ids = tf.concat([cls, tweets], axis=-1)\n",
    "\n",
    "lens = [len(i) for i in input_word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 117227,
     "status": "ok",
     "timestamp": 1602562290347,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "-Yb9EjKDTNhj",
    "outputId": "1e16e3fc-8dbc-410f-e7f7-d150afbf6882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is: 105\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max(lens)\n",
    "print('Max length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9_1dgmeZX5w"
   },
   "source": [
    "Most of the time I suggest adding a bit more to the `max_seq_length` esspecially when ussing just a fraction of data. Let's make it `1.5*max_seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 112082,
     "status": "ok",
     "timestamp": 1602562290351,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "1-MsePX3Zq0E",
    "outputId": "4a03138a-80e4-4ab3-f324-a1d7fa72cd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is: 157\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = int(1.5*max_seq_length)\n",
    "print('Max length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8-V_v61l1m_"
   },
   "source": [
    "**[NEW]** We need to encode our `wkd` classes and update our input functions to include that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jvodXTjwl_ac"
   },
   "outputs": [],
   "source": [
    "featureEncoder = LabelEncoder()\n",
    "featureEncoder.fit(wkd)\n",
    "\n",
    "# encode\n",
    "encoded_wkd_train = featureEncoder.transform(wkd_train)\n",
    "encoded_wkd_test = featureEncoder.transform(wkd_test)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_wkd_train = np_utils.to_categorical(encoded_wkd_train)\n",
    "dummy_wkd_test = np_utils.to_categorical(encoded_wkd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF9ncVA2J0Xh"
   },
   "source": [
    "**[NEW]** And once again save this encoder for later as we did with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "u0wC6sLDJrnD"
   },
   "outputs": [],
   "source": [
    "featureEncoder_fname = 'twitter_wkd.npy'\n",
    "np.save(os.path.join(my_wd, featureEncoder_fname) , featureEncoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doqSqtdET5JQ"
   },
   "source": [
    "Now we can make our inputs for train and test subsamples. There will be **[NEW]** as a comment to new lines which differ from our last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1603110922634,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "YYTddxJZT4cS"
   },
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list,\n",
    "                tokenizer, \n",
    "                new_feature,  # [NEW]\n",
    "                new_feature_class_count,  # [NEW] \n",
    "                max_seq_length):  \n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "  feature = tf.ragged.constant(new_feature).to_tensor(shape=(None, new_feature_class_count))  # [NEW]\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids,\n",
    "      'additional_feature': feature}  # [NEW]\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K_R_A3lULSO"
   },
   "source": [
    "**[NEW]** And now we preprocess inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3X6EQXxVUi4s"
   },
   "outputs": [],
   "source": [
    "feature_class_count = len(df.wkd.unique())\n",
    "X_train = bert_encode(x_train, tokenizer, dummy_wkd_train, feature_class_count, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, dummy_wkd_test, feature_class_count, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ6PVsxmVbRO"
   },
   "source": [
    "# Bam MODEL part already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QVhnrzH9ygq"
   },
   "source": [
    "## Initial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXvVljK3VjKt"
   },
   "source": [
    "We need to set up our model using the inputs we made, BERT model that we downloaded and an output layer based on num of classes we are using. Check for **[NEW]** lines to understand the addition to our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TENzkxMeThTv"
   },
   "outputs": [],
   "source": [
    "num_class = len(encoder.classes_)  # Based on available class selection\n",
    "max_seq_length = max_seq_length  # we calculated this a couple cells ago\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "feature_input = tf.keras.layers.Input(shape=(feature_class_count,),  # [NEW]\n",
    "                                      dtype=tf.float32, \n",
    "                                      name=\"additional_feature\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])    \n",
    "\n",
    "output = tf.keras.layers.concatenate([pooled_output, feature_input], name='pooled_with_additional')  # [NEW]\n",
    "\n",
    "output = tf.keras.layers.Dropout(rate=0.1)(output)  # [NEW]\n",
    "\n",
    "# additional dense layer here\n",
    "\n",
    "output = tf.keras.layers.Dense(num_class, activation='softmax', name='output')(output)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs={\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': segment_ids,\n",
    "        'additional_feature': feature_input  # [NEW]\n",
    "        },\n",
    "        outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4CLYDQYUZMP"
   },
   "source": [
    "Our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1602562492940,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "oN0Kh1ruUWPF",
    "outputId": "26564648-ccc9-482e-c09d-42bce9a1f9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXvpNMTop3sz"
   },
   "source": [
    "**[NEW]** As you can see from the plot above we have two additional layers:\n",
    "\n",
    "\n",
    "1.   `additional_feature` - our new input layer\n",
    "2.   `pooled_with_additional` - concatenated layer from `pooled_output` of BERT and our new `additional_feature`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvap4afcV4cZ"
   },
   "source": [
    "Set up the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bNeskVwpUco0"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 20  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = op.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2PRw94bV_cy"
   },
   "source": [
    "Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dpXu5QfFWAoD"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1602562509921,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "K0-u2YKQWDAV",
    "outputId": "9f70f3b3-a397-486c-ef90-816695251502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 157)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 157)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 157)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 768),        177853441   ['input_word_ids[0][0]',         \n",
      "                                 (None, 157, 768)]                'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " additional_feature (InputLayer  [(None, 7)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " pooled_with_additional (Concat  (None, 775)         0           ['keras_layer[0][0]',            \n",
      " enate)                                                           'additional_feature[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 775)          0           ['pooled_with_additional[0][0]'] \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            1552        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177,854,993\n",
      "Trainable params: 177,854,992\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_jIRI2zWbtx"
   },
   "source": [
    "Set up a history to check our model performance afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 6605829,
     "status": "ok",
     "timestamp": 1602569124582,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "ATMWQDkVWY6n",
    "outputId": "fc746b69-f8bf-45c1-b6c3-1efd7da8d18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    dummy_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, dummy_y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVMCMpQJXXdM"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 742335,
     "status": "ok",
     "timestamp": 1602569866964,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "6yY0h7CPXWD_",
    "outputId": "11a5f26a-b6af-4491-bc35-aca695c44c9a"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, dummy_y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeIFeUsyXVwQ"
   },
   "source": [
    "Noice plots are noice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1jJDOPzXcEH"
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 742820,
     "status": "ok",
     "timestamp": 1602569867475,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "RY7XA4HP9S7H",
    "outputId": "65756f02-6df1-4208-85b8-d798d1928140"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5VEZdft9buk"
   },
   "source": [
    "So, looking at the plot above we can see that at the 3rd epoch our `Validation Loss` jumped significantly even though `Validation Accuracy` stayed pretty much the same. If we would train it more on the same data it would significantly overtrain. \\\\\n",
    "What we can do instead is make another training dataset, lower our learning rate commonly denoted as `lr`, recompile our model using new `lr` and then train it a bit more. \\\\\n",
    "But before doing that we should save our model just in case if we would want to do the additional training in later sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-eqXo5f3SJc"
   },
   "source": [
    "## Model saving for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ART6DOetXit1"
   },
   "source": [
    "Let's save our model for later use. Unfortunatelly we can't use the optimizer we used for model training to save. I get an error and I can't resolve it. So we will recompile our model before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nInJE2SrXfS3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGLOM58PX4Rg"
   },
   "outputs": [],
   "source": [
    "# model_fname = 'twitter_BERT_wWKD'  # [NEW]\n",
    "\n",
    "# model.save(os.path.join(my_wd, model_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl6wQ47qah-I"
   },
   "source": [
    "### Validate saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCH6r6NnakdW"
   },
   "source": [
    "Sometimes TF likes to corrupt your model when saving. It's always a good idea to check if everything loads correctly after save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24929,
     "status": "ok",
     "timestamp": 1603110965171,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "RUc7jKm6acec"
   },
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(os.path.join(my_wd, model_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 914581,
     "status": "ok",
     "timestamp": 1602570039292,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "PTW3N0FuawQP",
    "outputId": "5c0db2c9-ae76-4f7c-cb8c-f271b1aafb38"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = new_model.evaluate(X_test, dummy_y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 914566,
     "status": "ok",
     "timestamp": 1602570039293,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "b41aPYSnayV_",
    "outputId": "0ed02d3e-3a2b-4845-fbf6-14d2ef600af8"
   },
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UResPCMRa2-Q"
   },
   "source": [
    "For safety reasons I would check the tokenizer variables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1724,
     "status": "ok",
     "timestamp": 1603110992725,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "9BOIJwPAazvD"
   },
   "outputs": [],
   "source": [
    "tokenizerSaved = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(my_wd, model_fname, 'assets/vocab.txt'),\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8_Z4Mxwbk10"
   },
   "source": [
    "If this loads, you should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "executionInfo": {
     "elapsed": 916593,
     "status": "ok",
     "timestamp": 1602570041344,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "iE3HggMgbkVO",
    "outputId": "fe0fc091-19b3-4154-ec2f-07fc63ab0525"
   },
   "outputs": [],
   "source": [
    "tokenizedTweet = tokenizerSaved.tokenize(x_train[0])\n",
    "for i in tokenizedTweet:\n",
    "  print(i, tokenizerSaved.convert_tokens_to_ids([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6JCf8ON-Hro"
   },
   "source": [
    "As everything seems fine, we can move on to second training itteration and try to make our model a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMhBzgkd5zAM"
   },
   "source": [
    "## Second training itteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEIaolS13gGY"
   },
   "source": [
    "As mentioned before, looking at the plot above we can see that at the 3rd epoch our `Validation Loss` jumped significantly even though `Validation Accuracy` stayed pretty much the same. If we would train it more on the same data it would significantly overtrain. \\\\\n",
    "\n",
    "What we can do instead is make another training dataset, lower our learning rate commonly denoted as `lr`, recompile our model using new `lr` and then train it a bit more.\n",
    "\n",
    "Are there any other options to approach overtraining? Sure. One of the most obvious ways would be to increase dropout layer number or the dropout precentage. You could try playing around with pooling as well, but in essence dropout would be your go to. \n",
    "\n",
    "If you do not know what dropout is then in short it 'turns off' preceeding layer neurons before next layer calculation, thus increasing the need for the model to 'abstract' things it sees.\n",
    "\n",
    "Anyway, in our case we are doing `lr` change and adding new data for the model. Let's see how that goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UotwRyBE6NP0"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TkMh3rG59LK"
   },
   "source": [
    "First we need to prep our data. It's good that we made our processing easier making some functions. Let's take another 5% sample from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXXGcaZM54hg"
   },
   "outputs": [],
   "source": [
    "sample_size = int(len(df)*0.05)\n",
    "sampleDf = df.sample(sample_size, random_state=42)\n",
    "wkd = sampleDf.wkd.values\n",
    "x = sampleDf.text.values\n",
    "y = sampleDf.target.values\n",
    "wkd_train2, wkd_test2, x_train2, x_test2, y_train2, y_test2 = train_test_split(wkd, x, y, test_size=0.20, random_state=23) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 916810,
     "status": "ok",
     "timestamp": 1602570041600,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "wC2tIF7-67F4",
    "outputId": "c9c4c943-8475-4e59-f7f7-64ca83b1036b"
   },
   "outputs": [],
   "source": [
    "classes = sampleDf.target.unique()\n",
    "print(classes)\n",
    "counts = []\n",
    "\n",
    "for i in classes:\n",
    "  count = len(sampleDf[sampleDf.target==i])\n",
    "  counts.append(count)\n",
    "\n",
    "plt.bar(['negative', 'positive'], counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Qh-nti6mR7"
   },
   "source": [
    "It's not a bad idea to always check if your sample is representitive of the dataset itself. Most of the time it will, but when getting small samples it might be off in small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAKNqpPh7c2M"
   },
   "source": [
    "#### Label Encoding\n",
    "Now we need to encode labels again. Good thing we have our label encoder saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teAtzXqt7ki-"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)\n",
    "\n",
    "encoded_Y_test2 = encoder.transform(y_test2)\n",
    "encoded_Y_train2 = encoder.transform(y_train2)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test2 = np_utils.to_categorical(encoded_Y_test2)\n",
    "dummy_y_train2 = np_utils.to_categorical(encoded_Y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c_9kIrI8cDz"
   },
   "source": [
    "#### Input preprocessing\n",
    "As we did before we need to tokenize our inputs (tweets) as `input_word_ids` and then add `input_mask` and `input_type`, as well as, `wkd` for weekday input. As we saved our model, we can use it to build our tokenizer as it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRDJu1jJ8mlF"
   },
   "outputs": [],
   "source": [
    "model_fname = 'twitter_BERT_wWKD'\n",
    "\n",
    "tokenizerSaved = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(my_wd, model_fname, 'assets/vocab.txt'),\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B939JjabLV96"
   },
   "source": [
    "And use our feature label encoder that we saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHkFBFqmLazU"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_wkd.npy'\n",
    "\n",
    "featureEncoderSaved = LabelEncoder()\n",
    "featureEncoderSaved.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)\n",
    "\n",
    "# encode\n",
    "encoded_wkd_train = featureEncoderSaved.transform(wkd_train2)\n",
    "encoded_wkd_test = featureEncoderSaved.transform(wkd_test2)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_wkd_train2 = np_utils.to_categorical(encoded_wkd_train)\n",
    "dummy_wkd_test2 = np_utils.to_categorical(encoded_wkd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rE5vM_OBaPP"
   },
   "source": [
    "I repaste our encoding functions here so it would be easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3158,
     "status": "ok",
     "timestamp": 1603107557490,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "wB9ImmkYbxJo"
   },
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list,\n",
    "                tokenizer, \n",
    "                new_feature,  # [NEW]\n",
    "                new_feature_class_count,  # [NEW] \n",
    "                max_seq_length):  \n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "  feature = tf.ragged.constant(new_feature).to_tensor(shape=(None, new_feature_class_count))  # [NEW]\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids,\n",
    "      'additional_feature': feature}  # [NEW]\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFSbqkIzCEjn"
   },
   "source": [
    "As you can see in the functions we use max_seq_length to convert our tweets into inputs. We can't really change it from the one used to build our model as it is in the model structure itself. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1602570815149,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "ACSBixa4CUg5",
    "outputId": "b65ca5dc-402e-4be0-f59b-c7353cc5149b"
   },
   "outputs": [],
   "source": [
    "print('Max sequence length is:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1602570817032,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "YoKZh0u0CcVT",
    "outputId": "3d120249-6ff3-43ae-f6ed-d8c05420eb4d"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7_D8CRsCpmM"
   },
   "source": [
    "You can see that all of the BERT `InputLayers` have a shape of (0, 240) which is (0, `max_seq_length`). For that reason we must use the same `max_seq_length` for our model to understand the inputs it gets. \n",
    "\n",
    "Also, the `additional_feature` input layer has a shape of (0, `feature_class_count`), where `feature_class_count` is 7 as in 7 days in the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMQbLqqpB2Zn"
   },
   "source": [
    "Now we will use the above functions to make all 4 of our inputs. Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiE8pkuqB6k_"
   },
   "outputs": [],
   "source": [
    "feature_class_count = 7  # days in week\n",
    "X_train2 = bert_encode(x_train2, tokenizerSaved, dummy_wkd_train2, feature_class_count, max_seq_length)\n",
    "X_test2 = bert_encode(x_test2, tokenizerSaved, dummy_wkd_test2, feature_class_count, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGHU8dopRKJU"
   },
   "source": [
    "So here we change only actual inputs of the model but not the contants set by our model architecture which are `feature_class_count` and `max_seq_length`. You could actually just input integers there instead of variables as we know them based on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXHn00cwDWac"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E04UInSQDkxM"
   },
   "source": [
    "Firstly let's load our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 12214,
     "status": "ok",
     "timestamp": 1602571006655,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "oIcOCiswDN9P",
    "outputId": "e4de079d-c09d-4c0d-f67c-252c2d0157cc"
   },
   "outputs": [],
   "source": [
    "model_fname = 'twitter_BERT_wWKD'\n",
    "\n",
    "new_model = tf.keras.models.load_model(os.path.join(my_wd, model_fname))\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1602571009548,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "yapkpJpXIFpw",
    "outputId": "fff5c316-3163-4a2a-b3b8-c5d6223282f4"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(new_model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = new_model.evaluate(X_test, dummy_y_test, verbose=False)  # OLD\n",
    "# print(\"Old testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "# loss, accuracy = new_model.evaluate(X_test2, dummy_y_test2, verbose=False)  # NEW\n",
    "# print(\"New testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "### NEW TESTING ACCURACY = 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAZri5-9D2Z3"
   },
   "source": [
    "To check training progress we can use the new and the old testing datasets. Their accuracy should be close. Let's see how it compares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoW6MxnIGRap"
   },
   "source": [
    "As you can see there's some difference. In any case, I suggest you use the same testing sample to proceed with you training no matter if the accuracy is the same. This of course applies only to when you do not train on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atw5HDRxGuBa"
   },
   "source": [
    "We need to setup our training parameters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfgAOC5KERoI"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 8  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_train2)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSWAB2FHLWA"
   },
   "source": [
    "Now the optimizer we use has learing rate decay in it. Which means learning rate goes down over time by the set parameters. We can lower the initial learning rate for this training session or we could use other optimizer with a fixed learning rate altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIKL2_TXHKNP"
   },
   "outputs": [],
   "source": [
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-6, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylQBZg5HpC3"
   },
   "source": [
    "Let's compile our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up0osQrxHntP"
   },
   "outputs": [],
   "source": [
    "new_model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YxVDhFMH86s"
   },
   "source": [
    "And now finally we can train again and see if that helps us achieve a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 2658919,
     "status": "ok",
     "timestamp": 1602578013184,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "H4_BM84kH03L",
    "outputId": "7b7f1225-8409-456a-df92-bd729eef5bdd"
   },
   "outputs": [],
   "source": [
    "# history2 = new_model.fit(X_train2,  # using new training set\n",
    "#                          dummy_y_train2,  # using new training set\n",
    "#                          epochs=epochs,\n",
    "#                          batch_size=batch_size,\n",
    "#                          validation_data=(X_test, dummy_y_test),  # using old test dataset\n",
    "#                          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6J-2ru2rKLo"
   },
   "source": [
    "Let's plot everything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHkRaP7Cpf_V"
   },
   "outputs": [],
   "source": [
    "# for i in history2.history:\n",
    "#   for ele in history2.history[i]:\n",
    "#     history.history[i].append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1602578013838,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "Ug8_LyTbqxcS",
    "outputId": "879fd325-a549-4d63-c772-6604289e4ff5"
   },
   "outputs": [],
   "source": [
    "#plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmWxjR2jrVjq"
   },
   "source": [
    "Our lines aren't consistent as we changed the training dataset. However, you can see that both the `val_accuracy` and `val_loss` improved. At his point we could itterate again and improve the model a bit more and more. Just understand that every incrimental improvement will take more data/time as it significantly slows down over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5y81Wvk1Cav"
   },
   "source": [
    "# Is BERT worth it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEzeXzX61GFe"
   },
   "source": [
    "So is BERT worth. BERT is a huge model compared to our simple NN and Logistic Regression models we used before. Is the increase in accuracy impactful enough compared to the increase in computation needed? Let's comapre accuracy:\n",
    "\n",
    "\n",
    "1.   Logistic Regression 0.80\n",
    "2.   Simple NN           0.79\n",
    "3.   Finetuned BERT      0.84\n",
    "\n",
    "In this case it totally depends on you. I personaly would say let's use Logistic Regression. Easily deployable, low computational resources and the task isn't that signicificant in terms of a mistake. If your goal is maximum accuracy possible I would train BERT a bit more and try to get to at least 85%, that would be significant enough to use BERT instead a much less computationaly intesnive Logistic Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIJMSxWRNOX"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvXzVKLNS3EZ"
   },
   "source": [
    "We need to check in with our label encoder to get our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6241,
     "status": "ok",
     "timestamp": 1603110965559,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "Q48W5efMSdtI"
   },
   "outputs": [],
   "source": [
    "encoder_fname = 'twitter_classes.npy'\n",
    "my_wd = '/home/jordan/rot/ent/tfBERT'\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)\n",
    "\n",
    "encoder_fname = 'twitter_wkd.npy'\n",
    "featureEncoderSaved = LabelEncoder()\n",
    "featureEncoderSaved.classes_ = np.load(os.path.join(my_wd, encoder_fname), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgjPsHVcTX9V"
   },
   "source": [
    "This is how our classes are encoded for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShPunnZvqSUO"
   },
   "source": [
    "And additional feature encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1603110970403,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "sNfSnaAkqZPJ",
    "outputId": "f5fe492f-c994-4c9a-ee19-54691642978a"
   },
   "outputs": [],
   "source": [
    "featureEncoderSaved.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QQWGEwy3K_i"
   },
   "source": [
    "Same with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1603111060326,
     "user": {
      "displayName": "Adam Ling",
      "photoUrl": "",
      "userId": "04744848912553865948"
     },
     "user_tz": -180
    },
    "id": "C8zwwthC3NNq"
   },
   "outputs": [],
   "source": [
    "model_fname = 'twitter_BERT_wWKD'\n",
    "\n",
    "tokenizerSaved = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(my_wd, model_fname, 'assets/vocab.txt'),\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import tweets for each asset\n",
    "\n",
    "### NLP Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_AAL = pd.read_csv('data/rated_AAL.csv')[['timestamp','tweet_text']]\n",
    "rated_GOOGL = pd.read_csv('data/rated_GOOGL.csv')[['timestamp','tweet_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP & Financial Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal = pd.read_csv('data/hashtagAAL.csv')[['timestamp','tweet_text']]\n",
    "btc = pd.read_csv('data/hashtagBTC-USD.csv')[['timestamp','tweet_text']]\n",
    "gme = pd.read_csv('data/hashtagGME.csv')[['timestamp','tweet_text']]\n",
    "spy = pd.read_csv('data/hashtagSPY.csv')[['timestamp','tweet_text']]\n",
    "wmt = pd.read_csv('data/hashtagWMT.csv')[['timestamp','tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    \n",
    "    newtweets = []\n",
    "    \n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %H:%M')\n",
    "    except:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #df['tweet_text'] = df['tweet_text'].apply(ast.literal_eval).str.decode(\"utf-8\")\n",
    "        \n",
    "    for j in df['tweet_text']:\n",
    "        j = j.replace(\"b'\",\"\")\n",
    "        j = j.replace('b\"',\"\")\n",
    "        if 'https' in j:\n",
    "            j = j[:j.find('https')]\n",
    "        newtweets.append(j)\n",
    "        \n",
    "    df['tweet_text'] = newtweets\n",
    " \n",
    "    #df['dow'] = df['timestamp'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data(aal)\n",
    "prep_data(btc)\n",
    "prep_data(gme)\n",
    "prep_data(spy)\n",
    "prep_data(wmt)\n",
    "prep_data(rated_AAL)\n",
    "prep_data(rated_GOOGL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sentiment Scores for Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tweets(df):\n",
    "    \n",
    "    # returns df of tweets with positivity score\n",
    "    # {'tweets', 'pred_sent'}\n",
    "    \n",
    "    preds_total = []\n",
    "    \n",
    "    for i in df.tweet_text:\n",
    "\n",
    "        wkd = ['Mon']\n",
    "        tweet = [str(i)]\n",
    "\n",
    "        dummy_wkd = np_utils.to_categorical(featureEncoderSaved.transform(np.array(wkd)))  # encodes weekday\n",
    "\n",
    "        inputs = bert_encode(string_list=list(tweet),\n",
    "                          tokenizer=tokenizerSaved,\n",
    "                          new_feature=dummy_wkd,\n",
    "                          new_feature_class_count=7,\n",
    "                          max_seq_length=157)\n",
    "\n",
    "\n",
    "        preds_total.append(float(new_model.predict(inputs)[0][1]))\n",
    "             \n",
    "    sent_tweet = {\n",
    "        'tweets': df['tweet_text'],\n",
    "        'pred_sentiment': preds_total          \n",
    "        \n",
    "    }\n",
    "    sent_tweet = pd.DataFrame(sent_tweet, columns = {'tweets', 'pred_sentiment'})\n",
    "\n",
    "\n",
    "    return sent_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AALsent = sent_tweets(rated_AAL)\n",
    "GOOGLsent = sent_tweets(rated_GOOGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data(btc)\n",
    "btc_sent = sent_tweets(btc[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_sent.to_csv('sentimentBTC_bert.csv') \n",
    "btc_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sentiment Scores for Financial Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_hour(df):\n",
    "    \n",
    "    # returns % positivity per hour\n",
    "    \n",
    "    preds_hours = []\n",
    "    \n",
    "    grp = df.groupby(by=[df.timestamp.map(lambda x : (x.hour))])\n",
    "\n",
    "    for i in grp.tweet_text:\n",
    "       \n",
    "        text = str(i[1]).split('\\n')\n",
    "        \n",
    "        tweets = []\n",
    "\n",
    "        for i in text:\n",
    "            tweets.append(i[5::])\n",
    "\n",
    "        preds_hour = []        \n",
    "\n",
    "        for i in tweets:\n",
    "        \n",
    "            wkd = ['Mon']\n",
    "            tweet = [str(i)]\n",
    "\n",
    "            dummy_wkd = np_utils.to_categorical(featureEncoderSaved.transform(np.array(wkd)))  # encodes weekday\n",
    "\n",
    "            inputs = bert_encode(string_list=list(tweet),\n",
    "                              tokenizer=tokenizerSaved,\n",
    "                              new_feature=dummy_wkd,\n",
    "                              new_feature_class_count=7,\n",
    "                              max_seq_length=157)\n",
    "            \n",
    "            preds_hours.append(float(new_model.predict(inputs)[0][1]))\n",
    "            \n",
    "   \n",
    "    \n",
    "    return preds_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_aal = sent_hour(aal)\n",
    "preds_btc = sent_hour(btc)\n",
    "preds_gme = sent_hour(gme)\n",
    "preds_spy = sent_hour(spy)\n",
    "preds_wmt = sent_hour(wmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial data to identify % change / hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(ticker, start, end):\n",
    "    \n",
    "    stock = yf.Ticker(f\"{ticker}\")\n",
    "\n",
    "    hist = stock.history(period=\"1d\",interval=\"1h\",start=start,end=end)\n",
    "    \n",
    "    hist.to_csv(f'data/{ticker}prices.csv')\n",
    "    \n",
    "for i in ['AAL','GME','SPY','WMT']:   \n",
    "    get_price(i,'2021-05-03','2021-05-04')\n",
    "    \n",
    "get_price('BTC-USD','2021-04-28','2021-04-29')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_AAL = pd.read_csv('data/AALprices.csv')\n",
    "prices_BTC = pd.read_csv('data/BTC-USDprices.csv')\n",
    "prices_GME = pd.read_csv('data/GMEprices.csv')\n",
    "prices_SPY = pd.read_csv('data/SPYprices.csv')\n",
    "prices_WMT = pd.read_csv('data/WMTprices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datax(delta,pred):\n",
    "    \n",
    "# pct change data available from 930 until 330 - first value NULL\n",
    "# preds has tweets for 24 hour day\n",
    "\n",
    "    pct = delta[1:7]\n",
    "    preds = pred[8:14]\n",
    "\n",
    "    newdf = {\n",
    "        'delta': delta[1:7],\n",
    "        'pred_sentiment': pred[8:14]         \n",
    "        \n",
    "    }\n",
    "    newdf = pd.DataFrame(newdf, columns = {'delta', 'pred_sentiment'})\n",
    "\n",
    "    newdf.to_csv('deltaWMT_bert.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_AAL = pd.read_csv('output/deltaAAL_bert.csv')\n",
    "d_GME = pd.read_csv('output/deltaGME_bert.csv')\n",
    "d_SPY = pd.read_csv('output/deltaSPY_bert.csv')\n",
    "d_WMT = pd.read_csv('output/deltaWMT_bert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction and Stock Δ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(df):\n",
    "    df = df.rename(columns = {\"Unnamed: 0\" : \"index\", \"pred_sentiment\" : \"pred\", \"delta\":\"real\"})\n",
    "    df = df[['real', 'pred']]\n",
    "    df.real = df.real*100\n",
    "    df.pred = df.pred\n",
    "    df.plot()\n",
    "    plt.xlabel('Stock Market Open Hours')\n",
    "    plt.ylabel('Δ')\n",
    "    \n",
    "#Gamestop was tanking and had difficult to interpret results, \n",
    "#however the hourly trend lines were apparent\n",
    "#acc(d_GME)\n",
    "acc(d_AAL) \n",
    "acc(d_SPY)\n",
    "acc(d_WMT)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP68cNXeWQIPX2Oau+GHxMl",
   "collapsed_sections": [],
   "mount_file_id": "1W9UZU-Yd4bEsh3ibztP4HUusj2X44ZPU",
   "name": "4_MODIFIED_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
